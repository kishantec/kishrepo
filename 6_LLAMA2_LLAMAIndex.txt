   
#need to install llama-index, openai, pypdf, python-dotenv


#RAG LLM APP WITH LLAMA2 and LLAMAIndex

#Load all pdf's from a folder

!pip install pypdf
!pip install -q transformers einops accelerate langchain bitsandbytes

#Embeddings
!pip install install sentence_transformers

!pip install llama-index

from llama_index import VectorStoreIndex,SimpleDirectoryReader
#VectorStoreIndex is for indexing purpose to retreive the documents faster

from llama_index import VectorStoreIndex,SimpleDirectoryReader,ServiceContext
from llama_index.llms import HuggingFaceLLM  #Im importing llama2 model from huggingface
from llama_index.prompts.prompts import SimpleInputPrompt #there are different types of prompts too

#ServiceContext = It is used to integrate the prompt template and llama Model

documents = SimpleDirectoryReader("/content/data").load_data()
documents

system_prompt=""" You are a Q&A assistant. Your goal is to answer questions as accurately as possible on the instructions and context provided"""

query_wrapper_prompt = SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

!!huggingface-cli login  #Wea re going to call the llama2.
#copy and paste the token

#Call llama2 model from hugging face

import torch

llm = HuggingFaceLLM(

context_window=4096,
max_new_tokens=256,
generate_kwargs={"temperature":0.0, "do_sample":False},
system_prompt=system_prompt,
query_wrapper_prompt = query_wrapper_prompt,
tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
model_name = "meta-llama/Lalama-2-7b-chat-hf",
device_map="auto",
model_kwargs={torch_dtype":torch.float16, "load_in_8bit":True}
)

from lanchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index import ServiceContext
from llama_index.embeddings import LanchainEmbedding

embed_model = LanchainEmbeddings(HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

service_context = ServiceContext.from_defaults(
  chunk_size = 1024,
  llm=llm,
  embed_model=embed_model 
)

index = VectorStoreIndex.from_documents(documents,service_context = service_context)
index

query_engine = index.as_query_engine()  // we can also do as chat engine

response = query_engine.query("what is attention is all you need?")

print(response)

#better way to show responses 2
from llama_index.response.pprint_utils import pprint_response
pprint_response(response,show_source=True)  #this will also show similarity score
print(response)

#We can create a different retriever and a query engine

from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.indices.postprocessor import SimilarityPostprocessor

retriever=VectorIndexRetriever(index=index,similarity_top_k=4)
postprocessor=SimilarityPostprocessor(similarity_cutoff=0.80)  #We can mention cut off for our similarity score
query_engine=RetrieverQueryEngine(retriever=retriever,node_postprocessor=postprocessor)

response=query_engine.query("what is...")

#Suppose I WANT TO STORE BY Vector Embeddings of my input source files(Indexes) in Disk and load them whenever i process them.

#Code is the same with some small updates

PERSIST_DIR = "./storage"
if not os.path.exists(PERSIST_DIR):
  documents = SimpleDirectoryReader("/content/data").load_data()
  index = VectorStoreIndex.from_documents(documents,service_context = service_context)
  index.storage_context.persist(persist_dir=PERSIST_DIR)
else:
  storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
  index = load_index_from_storage(storage_context)